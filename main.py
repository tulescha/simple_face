import cv2
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import pickle
import os
from PIL import Image
import time


class PyTorchFaceRecognition:
    def __init__(self, threshold=0.7):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.model = self.load_pretrained_model()

        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        # –î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü OpenCV
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

        self.threshold = threshold
        self.face_database = {}
        self.load_database()

    def load_pretrained_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ PyTorch"""
        try:
            model = models.resnet18(pretrained=True)
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (fc), –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º avgpool
            modules = list(model.children())[:-1]
            model = nn.Sequential(*modules)
            model.eval()
            model.to(self.device)
            print("‚úÖ PyTorch –º–æ–¥–µ–ª—å ResNet18 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return model
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None


    def extract_features_torch(self, face_image):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é PyTorch"""
        if self.model is None:
            return np.random.randn(512).astype(np.float32)

        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
            face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(face_rgb)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            input_tensor = self.transform(pil_image)
            input_batch = input_tensor.unsqueeze(0).to(self.device)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            with torch.no_grad():
                features = self.model(input_batch)  # [1, 512, 1, 1]
                features = features.view(features.size(0), -1)  # [1, 512]
                features = features.squeeze(0).cpu().numpy()  # (512,)

            print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {features.shape}")
            return features.astype(np.float32)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ PyTorch: {e}")
            return np.random.randn(512).astype(np.float32)

    def load_database(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ª–∏—Ü"""
        try:
            if os.path.exists('pytorch_face_database.pkl'):
                with open('pytorch_face_database.pkl', 'rb') as f:
                    self.face_database = pickle.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.face_database)} –ª–∏—Ü –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
            else:
                print("üìù –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å.")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.face_database = {}

    def save_database(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with open('pytorch_face_database.pkl', 'wb') as f:
                pickle.dump(self.face_database, f)
            print("üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

    def setup_camera(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞–º–µ—Ä—ã"""
        for camera_index in [0, 1]:
            cap = cv2.VideoCapture(camera_index)
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

            if cap.isOpened():
                ret, frame = cap.read()
                if ret and frame is not None:
                    print(f"‚úÖ –ö–∞–º–µ—Ä–∞ {camera_index} –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
                    return cap
            cap.release()

        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–º–µ—Ä–µ!")
        return None

    def detect_face(self, frame):
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü–∞"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(100, 100),
                flags=cv2.CASCADE_SCALE_IMAGE
            )

            if len(faces) > 0:
                x, y, w, h = faces[0]
                face_img = frame[y:y + h, x:x + w]
                return face_img, [x, y, x + w, y + h]

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü–∞: {e}")

        return None, None

    def calculate_cosine_similarity(self, features1, features2):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã
            norm1 = np.linalg.norm(features1)
            norm2 = np.linalg.norm(features2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = np.dot(features1, features2) / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏: {e}")
            return 0.0

    def register_face(self, name):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ª–∏—Ü–∞ —Å PyTorch"""
        print(f"\nüì∑ –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {name}")

        cap = self.setup_camera()
        if cap is None:
            return False

        print("üëÄ –°–º–æ—Ç—Ä–∏—Ç–µ –≤ –∫–∞–º–µ—Ä—É –ø—Ä—è–º–æ...")
        print("‚è≥ –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã...")

        features_list = []
        sample_count = 0
        max_samples = 5

        while sample_count < max_samples:
            ret, frame = cap.read()
            if not ret:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                continue

            face_img, box = self.detect_face(frame)
            display_frame = frame.copy()

            if face_img is not None:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é PyTorch
                features = self.extract_features_torch(face_img)
                features_list.append(features)
                sample_count += 1

                print(f"‚úÖ –û–±—Ä–∞–∑–µ—Ü {sample_count}/{max_samples} - –ü—Ä–∏–∑–Ω–∞–∫–∏: {features.shape}")

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                x1, y1, x2, y2 = box
                cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(display_frame, f"PyTorch Sample: {sample_count}", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(display_frame, f"Features: {features.shape[0]} dim", (10, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–∑—Ü–∞–º–∏
                time.sleep(1)
            else:
                cv2.putText(display_frame, "–õ–∏—Ü–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –¥–≤–∏–≥–∞–π—Ç–µ—Å—å –±–ª–∏–∂–µ", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

            cv2.imshow('PyTorch Face Registration - Press Q to cancel', display_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("‚ùå –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break

        cap.release()
        cv2.destroyAllWindows()

        if features_list:
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            avg_features = np.mean(features_list, axis=0)
            self.face_database[name] = avg_features
            self.save_database()
            print(f"\nüéâ –£–°–ü–ï–•! –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {name} –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω!")
            print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {avg_features.shape}")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return True
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–∏—Ü–æ - –ª–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
            return False

    def recognize_face(self, features):
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü–∞ –ø–æ PyTorch –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        if not self.face_database:
            return "No database", 0.0

        best_match = "Unknown"
        best_similarity = 0.0

        for name, stored_features in self.face_database.items():
            similarity = self.calculate_cosine_similarity(features, stored_features)

            if similarity > best_similarity and similarity > self.threshold:
                best_similarity = similarity
                best_match = name

        return best_match, best_similarity

    def real_time_recognition(self):
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å PyTorch"""
        if not self.face_database:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ª–∏—Ü–æ!")
            return

        cap = self.setup_camera()
        if cap is None:
            return

        print("\nüîç –ó–∞–ø—É—Å–∫ PyTorch —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è...")
        print("üìç –ù–∞–∂–º–∏—Ç–µ Q –¥–ª—è –≤—ã—Ö–æ–¥–∞")

        recognition_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                print("‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –∫–∞–¥—Ä–∞")
                break

            face_img, box = self.detect_face(frame)
            display_frame = frame.copy()

            if face_img is not None and box is not None:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é PyTorch
                features = self.extract_features_torch(face_img)
                name, confidence = self.recognize_face(features)

                recognition_count += 1

                x1, y1, x2, y2 = box

                if name != "Unknown":
                    color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π - —Å–≤–æ–π
                    status = f"‚úÖ {name}"
                    confidence_text = f"Confidence: {confidence:.3f}"
                else:
                    color = (0, 0, 255)  # –ö—Ä–∞—Å–Ω—ã–π - —á—É–∂–æ–π
                    status = "‚ùå Unknown Person"
                    confidence_text = f"Similarity: {confidence:.3f}"

                # –†–∏—Å—É–µ–º —Ä–∞–º–∫—É –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 3)
                cv2.putText(display_frame, status, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                cv2.putText(display_frame, confidence_text, (x1, y1 - 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

                # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                cv2.putText(display_frame, "PyTorch ResNet18", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(display_frame, f"Database: {len(self.face_database)} users",
                            (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(display_frame, f"Frame: {recognition_count}",
                            (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            else:
                cv2.putText(display_frame, "üîç Searching for face...", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(display_frame, "PyTorch Ready - Show your face", (10, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            cv2.imshow('PyTorch Face Recognition - Press Q to quit', display_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("üëã –í—ã—Ö–æ–¥ –∏–∑ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")
                break

        cap.release()
        cv2.destroyAllWindows()


def test_system():
    """–¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É...")

    # –¢–µ—Å—Ç –∫–∞–º–µ—Ä—ã
    cap = cv2.VideoCapture(0)
    if cap.isOpened():
        ret, frame = cap.read()
        if ret:
            print("‚úÖ –ö–∞–º–µ—Ä–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            # –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü–∞
            face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 5)
            print(f"‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(faces)}")
        cap.release()
    else:
        print("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")

    # –¢–µ—Å—Ç PyTorch
    try:
        model = models.resnet18(pretrained=True)
        print("‚úÖ PyTorch –∏ torchvision —Ä–∞–±–æ—Ç–∞—é—Ç")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ PyTorch: {e}")


def main():
    print("=" * 70)
    print("üîê PyTorch –°–ò–°–¢–ï–ú–ê –ë–ò–û–ú–ï–¢–†–ò–ß–ï–°–ö–û–ô –ò–î–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 70)
    print("üß† –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: ResNet18 + PyTorch + OpenCV")
    print("üéØ –ê–ª–≥–æ—Ä–∏—Ç–º: –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –≥–ª—É–±–æ–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print("üìä –ú–µ—Ç—Ä–∏–∫–∞: 512-–º–µ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
    print("=" * 70)

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    print(f"üì¶ PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"‚ö° CUDA –¥–æ—Å—Ç—É–ø–Ω–æ: {torch.cuda.is_available()}")
    print(f"üî¢ NumPy –≤–µ—Ä—Å–∏—è: {np.__version__}")
    print(f"üì∑ OpenCV –≤–µ—Ä—Å–∏—è: {cv2.__version__}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    test_system()

    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    recognizer = PyTorchFaceRecognition(threshold=0.6)

    while True:
        print("\n" + "=" * 50)
        print("–ú–ï–ù–Æ PyTorch —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:")
        print("1. üì∑ –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ–µ –ª–∏—Ü–æ")
        print("2. üîç –ó–∞–ø—É—Å–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        print("3. üë• –ü—Ä–æ—Å–º–æ—Ç—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        print("4. ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã")
        print("5. üö™ –í—ã—Ö–æ–¥")
        print("=" * 50)

        choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-5): ").strip()

        if choice == '1':
            name = input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ –∏–º—è: ").strip()
            if name:
                if name in recognizer.face_database:
                    print(f"‚ö†Ô∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {name} —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω!")
                    overwrite = input("–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å? (y/n): ").strip().lower()
                    if overwrite == 'y':
                        recognizer.register_face(name)
                else:
                    recognizer.register_face(name)
            else:
                print("‚ùå –ò–º—è –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")

        elif choice == '2':
            recognizer.real_time_recognition()

        elif choice == '3':
            if recognizer.face_database:
                print(f"\nüìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ({len(recognizer.face_database)}):")
                for i, name in enumerate(recognizer.face_database.keys(), 1):
                    features = recognizer.face_database[name]
                    print(f"   {i}. üë§ {name} - –ø—Ä–∏–∑–Ω–∞–∫–∏: {features.shape}")
            else:
                print("\nüìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞")

        elif choice == '4':
            print(f"\n‚öôÔ∏è –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
            print(f"   –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {recognizer.threshold}")
            print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {recognizer.device}")
            new_threshold = input("–ù–æ–≤—ã–π –ø–æ—Ä–æ–≥ (0.1-0.9) –∏–ª–∏ Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞: ").strip()
            if new_threshold:
                try:
                    recognizer.threshold = float(new_threshold)
                    print(f"‚úÖ –ü–æ—Ä–æ–≥ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {recognizer.threshold}")
                except ValueError:
                    print("‚ùå –ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞")

        elif choice == '5':
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –°–∏—Å—Ç–µ–º–∞ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É...")
            break

        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


if __name__ == "__main__":
    main()